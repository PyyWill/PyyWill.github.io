---
title:          "FLAME:Learning to Navigate with Multimodal LLM in Urban Environments"
date:           2024-12
selected:       true
pub:            "The 39th Annual AAAI Conference on Artificial Intelligence (AAAI)"
# pub_pre:        "Submitted to "
pub_post:       'Accepted (**Oral Presentation**).'
pub_last:       ' <span class="badge badge-pill badge-publication badge-success">Power Pitch</span>'
pub_date:       "2024"

abstract: >-
   Large Language Models(LLMs)havedemonstrated potential in Vision-and-Language Navigation (VLN) tasks, yet current applications face challenges. While LLMs excel in general conversation scenarios, they struggle with specialized navigation tasks, yielding suboptimal performance compared to specialized VLN models. We introduce FLAME (FLAMingoArchitected Embodied Agent), a novel Multimodal LLM-based agent and architecture designed for urban VLN tasks that efficiently handles multiple observations. Our approach implements a three-phase tuning technique for effective adaptation to navigation tasks, including single perception tuning for street view description, multiple perception tuning for route summarization, and end-to-end training on VLN datasets. The augmented datasets are synthesized automatically. Experimental results demonstrate FLAMEâ€™s superiority over existing methods, surpassing state-of-the-art methods by a 7.3% increase in task completion on Touchdown dataset. This work showcases the potential of Multimodal LLMs (MLLMs) in complex navigation tasks, representing an advancement towards applications of MLLMs in the field of embodied intelligence.
cover:          /assets/images/covers/cover_FLAME.png
authors:
  - Yunzhe Xu
  - '**Yiyuan Pan**'
  - Zhe Liu
  - Hesheng Wang
links:
  Paper: /assets/images/files/FLAME__Learning_to_Navigate_with_Multimodal_LLM_in_Urban_Environments.pdf
---
